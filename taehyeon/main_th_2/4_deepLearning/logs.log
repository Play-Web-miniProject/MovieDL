2023-04-03 18:41:03,731:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-03 18:41:03,733:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-03 18:41:03,733:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-03 18:41:03,733:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-04-03 18:41:07,940:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-04-03 18:41:37,218:INFO:PyCaret RegressionExperiment
2023-04-03 18:41:37,220:INFO:Logging name: reg-default-name
2023-04-03 18:41:37,220:INFO:ML Usecase: MLUsecase.REGRESSION
2023-04-03 18:41:37,220:INFO:version 3.0.0
2023-04-03 18:41:37,220:INFO:Initializing setup()
2023-04-03 18:41:37,220:INFO:self.USI: f258
2023-04-03 18:41:37,220:INFO:self._variable_keys: {'exp_name_log', 'fold_groups_param', 'gpu_param', 'fold_generator', 'X_test', 'idx', 'n_jobs_param', 'memory', 'seed', 'y', 'data', 'log_plots_param', '_available_plots', 'logging_param', '_ml_usecase', 'pipeline', 'y_test', 'transform_target_param', 'target_param', 'X_train', 'fold_shuffle_param', 'html_param', 'exp_id', 'gpu_n_jobs_param', 'USI', 'y_train', 'X'}
2023-04-03 18:41:37,220:INFO:Checking environment
2023-04-03 18:41:37,220:INFO:python_version: 3.9.13
2023-04-03 18:41:37,220:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-04-03 18:41:37,220:INFO:machine: AMD64
2023-04-03 18:41:37,220:INFO:platform: Windows-10-10.0.19044-SP0
2023-04-03 18:41:37,220:INFO:Memory: svmem(total=16889774080, available=7334797312, percent=56.6, used=9554976768, free=7334797312)
2023-04-03 18:41:37,220:INFO:Physical Core: 4
2023-04-03 18:41:37,221:INFO:Logical Core: 8
2023-04-03 18:41:37,221:INFO:Checking libraries
2023-04-03 18:41:37,221:INFO:System:
2023-04-03 18:41:37,221:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-04-03 18:41:37,221:INFO:executable: c:\Anaconda3\python.exe
2023-04-03 18:41:37,221:INFO:   machine: Windows-10-10.0.19044-SP0
2023-04-03 18:41:37,221:INFO:PyCaret required dependencies:
2023-04-03 18:41:37,221:INFO:                 pip: 22.2.2
2023-04-03 18:41:37,221:INFO:          setuptools: 63.4.1
2023-04-03 18:41:37,221:INFO:             pycaret: 3.0.0
2023-04-03 18:41:37,221:INFO:             IPython: 7.31.1
2023-04-03 18:41:37,221:INFO:          ipywidgets: 7.6.5
2023-04-03 18:41:37,221:INFO:                tqdm: 4.64.1
2023-04-03 18:41:37,221:INFO:               numpy: 1.21.5
2023-04-03 18:41:37,221:INFO:              pandas: 1.4.4
2023-04-03 18:41:37,221:INFO:              jinja2: 2.11.3
2023-04-03 18:41:37,221:INFO:               scipy: 1.9.1
2023-04-03 18:41:37,221:INFO:              joblib: 1.1.0
2023-04-03 18:41:37,222:INFO:             sklearn: 1.0.2
2023-04-03 18:41:37,222:INFO:                pyod: 1.0.9
2023-04-03 18:41:37,222:INFO:            imblearn: 0.10.1
2023-04-03 18:41:37,222:INFO:   category_encoders: 2.6.0
2023-04-03 18:41:37,222:INFO:            lightgbm: 3.3.5
2023-04-03 18:41:37,222:INFO:               numba: 0.55.1
2023-04-03 18:41:37,222:INFO:            requests: 2.28.1
2023-04-03 18:41:37,223:INFO:          matplotlib: 3.5.2
2023-04-03 18:41:37,223:INFO:          scikitplot: 0.3.7
2023-04-03 18:41:37,223:INFO:         yellowbrick: 1.5
2023-04-03 18:41:37,223:INFO:              plotly: 5.9.0
2023-04-03 18:41:37,223:INFO:             kaleido: 0.2.1
2023-04-03 18:41:37,223:INFO:         statsmodels: 0.13.2
2023-04-03 18:41:37,224:INFO:              sktime: 0.16.1
2023-04-03 18:41:37,224:INFO:               tbats: 1.1.2
2023-04-03 18:41:37,224:INFO:            pmdarima: 2.0.3
2023-04-03 18:41:37,224:INFO:              psutil: 5.9.0
2023-04-03 18:41:37,224:INFO:PyCaret optional dependencies:
2023-04-03 18:41:37,272:INFO:                shap: Not installed
2023-04-03 18:41:37,272:INFO:           interpret: Not installed
2023-04-03 18:41:37,272:INFO:                umap: Not installed
2023-04-03 18:41:37,272:INFO:    pandas_profiling: 4.1.1
2023-04-03 18:41:37,272:INFO:  explainerdashboard: Not installed
2023-04-03 18:41:37,272:INFO:             autoviz: Not installed
2023-04-03 18:41:37,272:INFO:           fairlearn: Not installed
2023-04-03 18:41:37,273:INFO:             xgboost: 1.7.4
2023-04-03 18:41:37,273:INFO:            catboost: Not installed
2023-04-03 18:41:37,273:INFO:              kmodes: Not installed
2023-04-03 18:41:37,273:INFO:             mlxtend: Not installed
2023-04-03 18:41:37,273:INFO:       statsforecast: Not installed
2023-04-03 18:41:37,273:INFO:        tune_sklearn: Not installed
2023-04-03 18:41:37,273:INFO:                 ray: Not installed
2023-04-03 18:41:37,273:INFO:            hyperopt: Not installed
2023-04-03 18:41:37,273:INFO:              optuna: Not installed
2023-04-03 18:41:37,273:INFO:               skopt: Not installed
2023-04-03 18:41:37,273:INFO:              mlflow: Not installed
2023-04-03 18:41:37,274:INFO:              gradio: Not installed
2023-04-03 18:41:37,274:INFO:             fastapi: Not installed
2023-04-03 18:41:37,274:INFO:             uvicorn: Not installed
2023-04-03 18:41:37,274:INFO:              m2cgen: Not installed
2023-04-03 18:41:37,274:INFO:           evidently: Not installed
2023-04-03 18:41:37,274:INFO:               fugue: Not installed
2023-04-03 18:41:37,274:INFO:           streamlit: Not installed
2023-04-03 18:41:37,274:INFO:             prophet: Not installed
2023-04-03 18:41:37,274:INFO:None
2023-04-03 18:41:37,274:INFO:Set up data.
2023-04-03 18:41:37,319:INFO:Set up train/test split.
2023-04-03 18:41:37,337:INFO:Set up index.
2023-04-03 18:41:37,337:INFO:Set up folding strategy.
2023-04-03 18:41:37,337:INFO:Assigning column types.
2023-04-03 18:41:37,344:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-03 18:41:37,344:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-03 18:41:37,349:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:37,354:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:37,434:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:37,489:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:37,490:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:37,777:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:37,777:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-03 18:41:37,784:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:37,788:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:37,875:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:37,932:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:37,933:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:37,936:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:37,936:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-04-03 18:41:37,943:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:37,950:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,033:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,086:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,087:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:38,090:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:38,097:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,101:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,179:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,235:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,236:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:38,239:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:38,239:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-04-03 18:41:38,254:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,322:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,377:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,379:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:38,384:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:38,397:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,473:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,540:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,541:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:38,546:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:38,547:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-04-03 18:41:38,751:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,827:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:38,828:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:38,831:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:38,946:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:39,006:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:39,006:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:39,010:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:39,011:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-03 18:41:39,095:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:39,148:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:39,150:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:39,240:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:39,296:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:39,299:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:39,299:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-04-03 18:41:39,437:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:39,440:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:39,579:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:39,582:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:39,587:INFO:Preparing preprocessing pipeline...
2023-04-03 18:41:39,587:INFO:Set up simple imputation.
2023-04-03 18:41:39,604:INFO:Set up encoding of ordinal features.
2023-04-03 18:41:39,605:INFO:Set up encoding of categorical features.
2023-04-03 18:41:39,606:INFO:Set up column name cleaning.
2023-04-03 18:41:40,169:INFO:Finished creating preprocessing pipeline.
2023-04-03 18:41:40,221:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Playdata\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Unnamed: 0', '순번', '전국 스크린수',
                                             '전국 매출액', '연도', '월', '총 관객수',
                                             '코로나', '평점', '감독_흥행', '배급사_흥행',
                                             '주연배우_흥행'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['영화명', '감독', '배급사', '개봉일', '영화형태',
                                             '국적', '서울 매출액',...
                                                                    '계절'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['영화명', '감독', '배급사', '개봉일',
                                             '서울 매출액', '서울 관객수', '출연'],
                                    transformer=LeaveOneOutEncoder(cols=['영화명',
                                                                         '감독',
                                                                         '배급사',
                                                                         '개봉일',
                                                                         '서울 '
                                                                         '매출액',
                                                                         '서울 '
                                                                         '관객수',
                                                                         '출연'],
                                                                   handle_missing='return_nan',
                                                                   random_state=580))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-04-03 18:41:40,221:INFO:Creating final display dataframe.
2023-04-03 18:41:41,286:INFO:Setup _display_container:                     Description             Value
0                    Session id               580
1                        Target            전국 관객수
2                   Target type        Regression
3           Original data shape        (1413, 26)
4        Transformed data shape        (1413, 60)
5   Transformed train set shape        (1130, 60)
6    Transformed test set shape         (283, 60)
7              Ordinal features                 1
8              Numeric features                12
9          Categorical features                13
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator             KFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  reg-default-name
22                          USI              f258
2023-04-03 18:41:41,509:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:41,515:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:41,777:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:41,785:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:41,787:INFO:setup() successfully completed in 4.57s...............
2023-04-03 18:41:46,813:INFO:PyCaret RegressionExperiment
2023-04-03 18:41:46,814:INFO:Logging name: reg-default-name
2023-04-03 18:41:46,814:INFO:ML Usecase: MLUsecase.REGRESSION
2023-04-03 18:41:46,814:INFO:version 3.0.0
2023-04-03 18:41:46,814:INFO:Initializing setup()
2023-04-03 18:41:46,814:INFO:self.USI: c44e
2023-04-03 18:41:46,814:INFO:self._variable_keys: {'exp_name_log', 'fold_groups_param', 'gpu_param', 'fold_generator', 'X_test', 'idx', 'n_jobs_param', 'memory', 'seed', 'y', 'data', 'log_plots_param', '_available_plots', 'logging_param', '_ml_usecase', 'pipeline', 'y_test', 'transform_target_param', 'target_param', 'X_train', 'fold_shuffle_param', 'html_param', 'exp_id', 'gpu_n_jobs_param', 'USI', 'y_train', 'X'}
2023-04-03 18:41:46,816:INFO:Checking environment
2023-04-03 18:41:46,816:INFO:python_version: 3.9.13
2023-04-03 18:41:46,816:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-04-03 18:41:46,816:INFO:machine: AMD64
2023-04-03 18:41:46,816:INFO:platform: Windows-10-10.0.19044-SP0
2023-04-03 18:41:46,816:INFO:Memory: svmem(total=16889774080, available=7339032576, percent=56.5, used=9550741504, free=7339032576)
2023-04-03 18:41:46,816:INFO:Physical Core: 4
2023-04-03 18:41:46,816:INFO:Logical Core: 8
2023-04-03 18:41:46,816:INFO:Checking libraries
2023-04-03 18:41:46,816:INFO:System:
2023-04-03 18:41:46,817:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-04-03 18:41:46,817:INFO:executable: c:\Anaconda3\python.exe
2023-04-03 18:41:46,817:INFO:   machine: Windows-10-10.0.19044-SP0
2023-04-03 18:41:46,817:INFO:PyCaret required dependencies:
2023-04-03 18:41:46,817:INFO:                 pip: 22.2.2
2023-04-03 18:41:46,817:INFO:          setuptools: 63.4.1
2023-04-03 18:41:46,817:INFO:             pycaret: 3.0.0
2023-04-03 18:41:46,817:INFO:             IPython: 7.31.1
2023-04-03 18:41:46,817:INFO:          ipywidgets: 7.6.5
2023-04-03 18:41:46,817:INFO:                tqdm: 4.64.1
2023-04-03 18:41:46,818:INFO:               numpy: 1.21.5
2023-04-03 18:41:46,818:INFO:              pandas: 1.4.4
2023-04-03 18:41:46,818:INFO:              jinja2: 2.11.3
2023-04-03 18:41:46,818:INFO:               scipy: 1.9.1
2023-04-03 18:41:46,818:INFO:              joblib: 1.1.0
2023-04-03 18:41:46,818:INFO:             sklearn: 1.0.2
2023-04-03 18:41:46,818:INFO:                pyod: 1.0.9
2023-04-03 18:41:46,819:INFO:            imblearn: 0.10.1
2023-04-03 18:41:46,819:INFO:   category_encoders: 2.6.0
2023-04-03 18:41:46,819:INFO:            lightgbm: 3.3.5
2023-04-03 18:41:46,819:INFO:               numba: 0.55.1
2023-04-03 18:41:46,819:INFO:            requests: 2.28.1
2023-04-03 18:41:46,820:INFO:          matplotlib: 3.5.2
2023-04-03 18:41:46,820:INFO:          scikitplot: 0.3.7
2023-04-03 18:41:46,820:INFO:         yellowbrick: 1.5
2023-04-03 18:41:46,820:INFO:              plotly: 5.9.0
2023-04-03 18:41:46,820:INFO:             kaleido: 0.2.1
2023-04-03 18:41:46,820:INFO:         statsmodels: 0.13.2
2023-04-03 18:41:46,820:INFO:              sktime: 0.16.1
2023-04-03 18:41:46,820:INFO:               tbats: 1.1.2
2023-04-03 18:41:46,820:INFO:            pmdarima: 2.0.3
2023-04-03 18:41:46,820:INFO:              psutil: 5.9.0
2023-04-03 18:41:46,821:INFO:PyCaret optional dependencies:
2023-04-03 18:41:46,821:INFO:                shap: Not installed
2023-04-03 18:41:46,821:INFO:           interpret: Not installed
2023-04-03 18:41:46,821:INFO:                umap: Not installed
2023-04-03 18:41:46,821:INFO:    pandas_profiling: 4.1.1
2023-04-03 18:41:46,821:INFO:  explainerdashboard: Not installed
2023-04-03 18:41:46,822:INFO:             autoviz: Not installed
2023-04-03 18:41:46,822:INFO:           fairlearn: Not installed
2023-04-03 18:41:46,822:INFO:             xgboost: 1.7.4
2023-04-03 18:41:46,822:INFO:            catboost: Not installed
2023-04-03 18:41:46,822:INFO:              kmodes: Not installed
2023-04-03 18:41:46,822:INFO:             mlxtend: Not installed
2023-04-03 18:41:46,823:INFO:       statsforecast: Not installed
2023-04-03 18:41:46,823:INFO:        tune_sklearn: Not installed
2023-04-03 18:41:46,823:INFO:                 ray: Not installed
2023-04-03 18:41:46,823:INFO:            hyperopt: Not installed
2023-04-03 18:41:46,823:INFO:              optuna: Not installed
2023-04-03 18:41:46,823:INFO:               skopt: Not installed
2023-04-03 18:41:46,823:INFO:              mlflow: Not installed
2023-04-03 18:41:46,823:INFO:              gradio: Not installed
2023-04-03 18:41:46,824:INFO:             fastapi: Not installed
2023-04-03 18:41:46,824:INFO:             uvicorn: Not installed
2023-04-03 18:41:46,824:INFO:              m2cgen: Not installed
2023-04-03 18:41:46,824:INFO:           evidently: Not installed
2023-04-03 18:41:46,824:INFO:               fugue: Not installed
2023-04-03 18:41:46,824:INFO:           streamlit: Not installed
2023-04-03 18:41:46,824:INFO:             prophet: Not installed
2023-04-03 18:41:46,824:INFO:None
2023-04-03 18:41:46,824:INFO:Set up data.
2023-04-03 18:41:46,852:INFO:Set up train/test split.
2023-04-03 18:41:46,869:INFO:Set up index.
2023-04-03 18:41:46,870:INFO:Set up folding strategy.
2023-04-03 18:41:46,870:INFO:Assigning column types.
2023-04-03 18:41:46,875:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-03 18:41:46,876:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-03 18:41:46,882:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:46,888:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:46,962:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,017:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,017:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:47,021:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:47,022:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,028:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,034:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,119:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,201:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,202:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:47,205:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:47,206:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-04-03 18:41:47,215:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,221:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,308:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,365:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,366:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:47,370:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:47,376:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,383:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,451:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,510:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,511:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:47,515:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:47,515:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-04-03 18:41:47,526:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,602:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,666:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,667:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:47,669:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:47,683:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,755:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,808:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,809:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:47,813:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:47,813:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-04-03 18:41:47,890:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,943:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:47,944:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:47,949:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:48,030:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:48,082:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:48,082:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:48,086:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:48,086:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-03 18:41:48,167:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:48,218:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:48,221:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:48,344:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:48,434:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:48,438:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:48,438:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-04-03 18:41:48,606:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:48,610:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:48,738:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:48,741:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:48,743:INFO:Preparing preprocessing pipeline...
2023-04-03 18:41:48,743:INFO:Set up simple imputation.
2023-04-03 18:41:48,749:INFO:Set up encoding of ordinal features.
2023-04-03 18:41:48,752:INFO:Set up encoding of categorical features.
2023-04-03 18:41:48,753:INFO:Set up column name cleaning.
2023-04-03 18:41:49,296:INFO:Finished creating preprocessing pipeline.
2023-04-03 18:41:49,336:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Playdata\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Unnamed: 0', '순번', '전국 스크린수',
                                             '전국 매출액', '연도', '월', '총 관객수',
                                             '코로나', '평점', '감독_흥행', '배급사_흥행',
                                             '주연배우_흥행'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['영화명', '감독', '배급사', '개봉일', '영화형태',
                                             '국적', '서울 매출액',...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['영화명', '감독', '배급사', '개봉일',
                                             '서울 매출액', '서울 관객수', '출연'],
                                    transformer=LeaveOneOutEncoder(cols=['영화명',
                                                                         '감독',
                                                                         '배급사',
                                                                         '개봉일',
                                                                         '서울 '
                                                                         '매출액',
                                                                         '서울 '
                                                                         '관객수',
                                                                         '출연'],
                                                                   handle_missing='return_nan',
                                                                   random_state=8895))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-04-03 18:41:49,336:INFO:Creating final display dataframe.
2023-04-03 18:41:50,538:INFO:Setup _display_container:                     Description             Value
0                    Session id              8895
1                        Target            전국 관객수
2                   Target type        Regression
3           Original data shape        (1413, 26)
4        Transformed data shape        (1413, 59)
5   Transformed train set shape        (1130, 59)
6    Transformed test set shape         (283, 59)
7              Ordinal features                 1
8              Numeric features                12
9          Categorical features                13
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator             KFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  reg-default-name
22                          USI              c44e
2023-04-03 18:41:50,795:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:50,800:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:50,986:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:50,989:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:50,989:INFO:setup() successfully completed in 4.18s...............
2023-04-03 18:41:58,223:INFO:PyCaret RegressionExperiment
2023-04-03 18:41:58,224:INFO:Logging name: reg-default-name
2023-04-03 18:41:58,224:INFO:ML Usecase: MLUsecase.REGRESSION
2023-04-03 18:41:58,224:INFO:version 3.0.0
2023-04-03 18:41:58,224:INFO:Initializing setup()
2023-04-03 18:41:58,224:INFO:self.USI: 1320
2023-04-03 18:41:58,225:INFO:self._variable_keys: {'exp_name_log', 'fold_groups_param', 'gpu_param', 'fold_generator', 'X_test', 'idx', 'n_jobs_param', 'memory', 'seed', 'y', 'data', 'log_plots_param', '_available_plots', 'logging_param', '_ml_usecase', 'pipeline', 'y_test', 'transform_target_param', 'target_param', 'X_train', 'fold_shuffle_param', 'html_param', 'exp_id', 'gpu_n_jobs_param', 'USI', 'y_train', 'X'}
2023-04-03 18:41:58,225:INFO:Checking environment
2023-04-03 18:41:58,225:INFO:python_version: 3.9.13
2023-04-03 18:41:58,226:INFO:python_build: ('main', 'Aug 25 2022 23:51:50')
2023-04-03 18:41:58,226:INFO:machine: AMD64
2023-04-03 18:41:58,226:INFO:platform: Windows-10-10.0.19044-SP0
2023-04-03 18:41:58,226:INFO:Memory: svmem(total=16889774080, available=7343374336, percent=56.5, used=9546399744, free=7343374336)
2023-04-03 18:41:58,226:INFO:Physical Core: 4
2023-04-03 18:41:58,226:INFO:Logical Core: 8
2023-04-03 18:41:58,227:INFO:Checking libraries
2023-04-03 18:41:58,227:INFO:System:
2023-04-03 18:41:58,227:INFO:    python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]
2023-04-03 18:41:58,227:INFO:executable: c:\Anaconda3\python.exe
2023-04-03 18:41:58,227:INFO:   machine: Windows-10-10.0.19044-SP0
2023-04-03 18:41:58,228:INFO:PyCaret required dependencies:
2023-04-03 18:41:58,228:INFO:                 pip: 22.2.2
2023-04-03 18:41:58,228:INFO:          setuptools: 63.4.1
2023-04-03 18:41:58,228:INFO:             pycaret: 3.0.0
2023-04-03 18:41:58,228:INFO:             IPython: 7.31.1
2023-04-03 18:41:58,228:INFO:          ipywidgets: 7.6.5
2023-04-03 18:41:58,228:INFO:                tqdm: 4.64.1
2023-04-03 18:41:58,228:INFO:               numpy: 1.21.5
2023-04-03 18:41:58,228:INFO:              pandas: 1.4.4
2023-04-03 18:41:58,229:INFO:              jinja2: 2.11.3
2023-04-03 18:41:58,229:INFO:               scipy: 1.9.1
2023-04-03 18:41:58,229:INFO:              joblib: 1.1.0
2023-04-03 18:41:58,229:INFO:             sklearn: 1.0.2
2023-04-03 18:41:58,229:INFO:                pyod: 1.0.9
2023-04-03 18:41:58,229:INFO:            imblearn: 0.10.1
2023-04-03 18:41:58,229:INFO:   category_encoders: 2.6.0
2023-04-03 18:41:58,230:INFO:            lightgbm: 3.3.5
2023-04-03 18:41:58,230:INFO:               numba: 0.55.1
2023-04-03 18:41:58,230:INFO:            requests: 2.28.1
2023-04-03 18:41:58,230:INFO:          matplotlib: 3.5.2
2023-04-03 18:41:58,230:INFO:          scikitplot: 0.3.7
2023-04-03 18:41:58,230:INFO:         yellowbrick: 1.5
2023-04-03 18:41:58,230:INFO:              plotly: 5.9.0
2023-04-03 18:41:58,230:INFO:             kaleido: 0.2.1
2023-04-03 18:41:58,230:INFO:         statsmodels: 0.13.2
2023-04-03 18:41:58,230:INFO:              sktime: 0.16.1
2023-04-03 18:41:58,230:INFO:               tbats: 1.1.2
2023-04-03 18:41:58,230:INFO:            pmdarima: 2.0.3
2023-04-03 18:41:58,231:INFO:              psutil: 5.9.0
2023-04-03 18:41:58,231:INFO:PyCaret optional dependencies:
2023-04-03 18:41:58,231:INFO:                shap: Not installed
2023-04-03 18:41:58,231:INFO:           interpret: Not installed
2023-04-03 18:41:58,231:INFO:                umap: Not installed
2023-04-03 18:41:58,231:INFO:    pandas_profiling: 4.1.1
2023-04-03 18:41:58,231:INFO:  explainerdashboard: Not installed
2023-04-03 18:41:58,231:INFO:             autoviz: Not installed
2023-04-03 18:41:58,231:INFO:           fairlearn: Not installed
2023-04-03 18:41:58,231:INFO:             xgboost: 1.7.4
2023-04-03 18:41:58,231:INFO:            catboost: Not installed
2023-04-03 18:41:58,231:INFO:              kmodes: Not installed
2023-04-03 18:41:58,231:INFO:             mlxtend: Not installed
2023-04-03 18:41:58,231:INFO:       statsforecast: Not installed
2023-04-03 18:41:58,231:INFO:        tune_sklearn: Not installed
2023-04-03 18:41:58,231:INFO:                 ray: Not installed
2023-04-03 18:41:58,231:INFO:            hyperopt: Not installed
2023-04-03 18:41:58,231:INFO:              optuna: Not installed
2023-04-03 18:41:58,231:INFO:               skopt: Not installed
2023-04-03 18:41:58,231:INFO:              mlflow: Not installed
2023-04-03 18:41:58,232:INFO:              gradio: Not installed
2023-04-03 18:41:58,232:INFO:             fastapi: Not installed
2023-04-03 18:41:58,232:INFO:             uvicorn: Not installed
2023-04-03 18:41:58,232:INFO:              m2cgen: Not installed
2023-04-03 18:41:58,232:INFO:           evidently: Not installed
2023-04-03 18:41:58,232:INFO:               fugue: Not installed
2023-04-03 18:41:58,232:INFO:           streamlit: Not installed
2023-04-03 18:41:58,232:INFO:             prophet: Not installed
2023-04-03 18:41:58,232:INFO:None
2023-04-03 18:41:58,232:INFO:Set up data.
2023-04-03 18:41:58,262:INFO:Set up train/test split.
2023-04-03 18:41:58,347:INFO:Set up index.
2023-04-03 18:41:58,347:INFO:Set up folding strategy.
2023-04-03 18:41:58,347:INFO:Assigning column types.
2023-04-03 18:41:58,356:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-04-03 18:41:58,357:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,397:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,409:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,515:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,604:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,605:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:58,612:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:58,614:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,623:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,633:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,716:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,771:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,772:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:58,774:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:58,775:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-04-03 18:41:58,784:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,790:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,875:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,938:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,938:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:58,941:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:58,949:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-04-03 18:41:58,955:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,055:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,118:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,119:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:59,122:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:59,122:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-04-03 18:41:59,134:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,240:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,323:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,325:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:59,330:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:59,348:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,461:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,523:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,524:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:59,531:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:59,531:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-04-03 18:41:59,624:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,695:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,696:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:59,699:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:59,785:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,835:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,835:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:59,838:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:41:59,838:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-04-03 18:41:59,922:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:41:59,981:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:41:59,983:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:42:00,066:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-04-03 18:42:00,123:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:42:00,129:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:42:00,130:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-04-03 18:42:00,270:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:42:00,273:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:42:00,421:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:42:00,424:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:42:00,427:INFO:Preparing preprocessing pipeline...
2023-04-03 18:42:00,427:INFO:Set up simple imputation.
2023-04-03 18:42:00,434:INFO:Set up encoding of ordinal features.
2023-04-03 18:42:00,435:INFO:Set up encoding of categorical features.
2023-04-03 18:42:00,436:INFO:Set up column name cleaning.
2023-04-03 18:42:01,115:INFO:Finished creating preprocessing pipeline.
2023-04-03 18:42:01,151:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Playdata\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Unnamed: 0', '순번', '전국 스크린수',
                                             '전국 매출액', '연도', '월', '총 관객수',
                                             '코로나', '평점', '감독_흥행', '배급사_흥행',
                                             '주연배우_흥행'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['영화명', '감독', '배급사', '개봉일', '영화형태',
                                             '국적', '서울 매출액',...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['영화명', '감독', '배급사', '개봉일',
                                             '서울 매출액', '서울 관객수', '출연'],
                                    transformer=LeaveOneOutEncoder(cols=['영화명',
                                                                         '감독',
                                                                         '배급사',
                                                                         '개봉일',
                                                                         '서울 '
                                                                         '매출액',
                                                                         '서울 '
                                                                         '관객수',
                                                                         '출연'],
                                                                   handle_missing='return_nan',
                                                                   random_state=1797))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-04-03 18:42:01,151:INFO:Creating final display dataframe.
2023-04-03 18:42:02,172:INFO:Setup _display_container:                     Description             Value
0                    Session id              1797
1                        Target            전국 관객수
2                   Target type        Regression
3           Original data shape        (1413, 26)
4        Transformed data shape        (1413, 60)
5   Transformed train set shape        (1130, 60)
6    Transformed test set shape         (283, 60)
7              Ordinal features                 1
8              Numeric features                12
9          Categorical features                13
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator             KFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  reg-default-name
22                          USI              1320
2023-04-03 18:42:02,436:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:42:02,449:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:42:02,637:INFO:Soft dependency imported: xgboost: 1.7.4
2023-04-03 18:42:02,641:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-04-03 18:42:02,641:INFO:setup() successfully completed in 4.43s...............
2023-04-03 18:42:39,105:INFO:Initializing compare_models()
2023-04-03 18:42:39,106:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, include=None, fold=10, round=4, cross_validation=True, sort=RMSE, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, 'include': None, 'exclude': None, 'fold': 10, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-04-03 18:42:39,106:INFO:Checking exceptions
2023-04-03 18:42:39,111:INFO:Preparing display monitor
2023-04-03 18:42:39,188:INFO:Initializing Linear Regression
2023-04-03 18:42:39,189:INFO:Total runtime is 1.6887982686360676e-05 minutes
2023-04-03 18:42:39,201:INFO:SubProcess create_model() called ==================================
2023-04-03 18:42:39,202:INFO:Initializing create_model()
2023-04-03 18:42:39,202:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:42:39,203:INFO:Checking exceptions
2023-04-03 18:42:39,203:INFO:Importing libraries
2023-04-03 18:42:39,203:INFO:Copying training dataset
2023-04-03 18:42:39,217:INFO:Defining folds
2023-04-03 18:42:39,217:INFO:Declaring metric variables
2023-04-03 18:42:39,225:INFO:Importing untrained model
2023-04-03 18:42:39,231:INFO:Linear Regression Imported successfully
2023-04-03 18:42:39,244:INFO:Starting cross validation
2023-04-03 18:42:39,272:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:07,222:INFO:Calculating mean and std
2023-04-03 18:43:07,225:INFO:Creating metrics dataframe
2023-04-03 18:43:07,275:INFO:Uploading results into container
2023-04-03 18:43:07,276:INFO:Uploading model into container now
2023-04-03 18:43:07,276:INFO:_master_model_container: 1
2023-04-03 18:43:07,276:INFO:_display_container: 2
2023-04-03 18:43:07,277:INFO:LinearRegression(n_jobs=-1)
2023-04-03 18:43:07,277:INFO:create_model() successfully completed......................................
2023-04-03 18:43:07,471:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:07,471:INFO:Creating metrics dataframe
2023-04-03 18:43:07,481:INFO:Initializing Lasso Regression
2023-04-03 18:43:07,481:INFO:Total runtime is 0.47155277331670126 minutes
2023-04-03 18:43:07,488:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:07,489:INFO:Initializing create_model()
2023-04-03 18:43:07,489:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:07,489:INFO:Checking exceptions
2023-04-03 18:43:07,490:INFO:Importing libraries
2023-04-03 18:43:07,490:INFO:Copying training dataset
2023-04-03 18:43:07,500:INFO:Defining folds
2023-04-03 18:43:07,501:INFO:Declaring metric variables
2023-04-03 18:43:07,506:INFO:Importing untrained model
2023-04-03 18:43:07,512:INFO:Lasso Regression Imported successfully
2023-04-03 18:43:07,528:INFO:Starting cross validation
2023-04-03 18:43:07,535:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:08,447:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+09, tolerance: 6.355e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:08,522:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.586e+09, tolerance: 6.245e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:08,629:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.616e+09, tolerance: 6.320e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:08,655:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.574e+09, tolerance: 6.420e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:08,686:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.801e+09, tolerance: 6.408e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:10,227:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.413e+09, tolerance: 6.220e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:10,310:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.786e+09, tolerance: 6.227e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:10,747:INFO:Calculating mean and std
2023-04-03 18:43:10,750:INFO:Creating metrics dataframe
2023-04-03 18:43:10,832:INFO:Uploading results into container
2023-04-03 18:43:10,832:INFO:Uploading model into container now
2023-04-03 18:43:10,834:INFO:_master_model_container: 2
2023-04-03 18:43:10,834:INFO:_display_container: 2
2023-04-03 18:43:10,835:INFO:Lasso(random_state=1797)
2023-04-03 18:43:10,835:INFO:create_model() successfully completed......................................
2023-04-03 18:43:11,005:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:11,005:INFO:Creating metrics dataframe
2023-04-03 18:43:11,016:INFO:Initializing Ridge Regression
2023-04-03 18:43:11,017:INFO:Total runtime is 0.5304822524388632 minutes
2023-04-03 18:43:11,024:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:11,024:INFO:Initializing create_model()
2023-04-03 18:43:11,026:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:11,026:INFO:Checking exceptions
2023-04-03 18:43:11,026:INFO:Importing libraries
2023-04-03 18:43:11,026:INFO:Copying training dataset
2023-04-03 18:43:11,036:INFO:Defining folds
2023-04-03 18:43:11,037:INFO:Declaring metric variables
2023-04-03 18:43:11,044:INFO:Importing untrained model
2023-04-03 18:43:11,049:INFO:Ridge Regression Imported successfully
2023-04-03 18:43:11,062:INFO:Starting cross validation
2023-04-03 18:43:11,069:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:11,868:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.16769e-20): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-03 18:43:11,878:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.12172e-20): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-03 18:43:11,971:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.16377e-20): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-03 18:43:12,003:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.14835e-20): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-03 18:43:12,044:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.15965e-20): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-03 18:43:12,044:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.13517e-20): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-03 18:43:12,082:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.13301e-20): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-03 18:43:12,369:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.12629e-20): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-03 18:43:13,349:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.44689e-20): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-03 18:43:13,354:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.17567e-20): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-04-03 18:43:13,842:INFO:Calculating mean and std
2023-04-03 18:43:13,845:INFO:Creating metrics dataframe
2023-04-03 18:43:13,927:INFO:Uploading results into container
2023-04-03 18:43:13,928:INFO:Uploading model into container now
2023-04-03 18:43:13,929:INFO:_master_model_container: 3
2023-04-03 18:43:13,929:INFO:_display_container: 2
2023-04-03 18:43:13,929:INFO:Ridge(random_state=1797)
2023-04-03 18:43:13,929:INFO:create_model() successfully completed......................................
2023-04-03 18:43:14,115:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:14,115:INFO:Creating metrics dataframe
2023-04-03 18:43:14,132:INFO:Initializing Elastic Net
2023-04-03 18:43:14,132:INFO:Total runtime is 0.5823984702428182 minutes
2023-04-03 18:43:14,140:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:14,140:INFO:Initializing create_model()
2023-04-03 18:43:14,140:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:14,140:INFO:Checking exceptions
2023-04-03 18:43:14,141:INFO:Importing libraries
2023-04-03 18:43:14,141:INFO:Copying training dataset
2023-04-03 18:43:14,154:INFO:Defining folds
2023-04-03 18:43:14,154:INFO:Declaring metric variables
2023-04-03 18:43:14,160:INFO:Importing untrained model
2023-04-03 18:43:14,167:INFO:Elastic Net Imported successfully
2023-04-03 18:43:14,184:INFO:Starting cross validation
2023-04-03 18:43:14,190:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:14,997:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+09, tolerance: 6.364e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:15,030:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.116e+09, tolerance: 6.245e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:15,036:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.273e+09, tolerance: 6.355e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:15,109:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.272e+09, tolerance: 6.408e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:15,114:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.068e+09, tolerance: 6.401e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:15,262:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.017e+09, tolerance: 6.420e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:15,507:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.093e+09, tolerance: 6.320e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:15,545:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.183e+09, tolerance: 6.315e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:16,665:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.261e+09, tolerance: 6.220e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:16,696:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.120e+09, tolerance: 6.227e+07
  model = cd_fast.enet_coordinate_descent(

2023-04-03 18:43:17,209:INFO:Calculating mean and std
2023-04-03 18:43:17,211:INFO:Creating metrics dataframe
2023-04-03 18:43:17,299:INFO:Uploading results into container
2023-04-03 18:43:17,300:INFO:Uploading model into container now
2023-04-03 18:43:17,302:INFO:_master_model_container: 4
2023-04-03 18:43:17,302:INFO:_display_container: 2
2023-04-03 18:43:17,303:INFO:ElasticNet(random_state=1797)
2023-04-03 18:43:17,303:INFO:create_model() successfully completed......................................
2023-04-03 18:43:17,473:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:17,473:INFO:Creating metrics dataframe
2023-04-03 18:43:17,482:INFO:Initializing Least Angle Regression
2023-04-03 18:43:17,483:INFO:Total runtime is 0.6382459084192912 minutes
2023-04-03 18:43:17,492:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:17,493:INFO:Initializing create_model()
2023-04-03 18:43:17,493:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:17,494:INFO:Checking exceptions
2023-04-03 18:43:17,494:INFO:Importing libraries
2023-04-03 18:43:17,494:INFO:Copying training dataset
2023-04-03 18:43:17,502:INFO:Defining folds
2023-04-03 18:43:17,502:INFO:Declaring metric variables
2023-04-03 18:43:17,509:INFO:Importing untrained model
2023-04-03 18:43:17,516:INFO:Least Angle Regression Imported successfully
2023-04-03 18:43:17,531:INFO:Starting cross validation
2023-04-03 18:43:17,538:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:18,299:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:18,302:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:18,343:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:18,356:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=5.812e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,356:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.471e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,357:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.221e+00, with an active set of 44 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,357:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=3.111e+01, with an active set of 49 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,358:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.140e+00, with an active set of 46 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,359:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.013e+00, with an active set of 46 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,359:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.186e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,360:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=8.223e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,360:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=7.630e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,361:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=5.989e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,361:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=3.002e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,361:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.347e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,362:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=7.242e+00, with an active set of 55 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,362:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=4.123e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,362:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=3.889e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,362:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.023e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,362:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.091e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,362:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=5.575e+03, with an active set of 50 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,363:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.404e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,363:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.806e+03, with an active set of 51 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,363:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.652e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,364:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=7.822e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,364:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.790e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,365:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.856e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,365:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.111e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,365:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.255e+00, with an active set of 41 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,375:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=8.507e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,377:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=8.202e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,378:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=5.106e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,378:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=5.051e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,378:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=3.513e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,379:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.353e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,379:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.181e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,379:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.073e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,380:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=8.161e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,380:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=5.179e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,381:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=4.098e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,381:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=3.723e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,398:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:18,422:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=9.414e-01, with an active set of 48 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,423:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=6.064e-01, with an active set of 48 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,424:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.375e+00, with an active set of 42 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,424:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=4.469e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,425:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.503e-01, with an active set of 45 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,425:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.540e+00, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,426:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.575e-01, with an active set of 45 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,426:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=4.080e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,426:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=6.485e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,427:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=5.578e-01, with an active set of 47 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,427:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=5.595e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,428:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=4.884e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,428:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=8.607e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,428:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.222e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,428:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=8.352e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,429:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.824e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 8.625e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,429:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=2.066e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,430:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=2.058e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.625e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,430:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.409e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,431:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=5.868e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,432:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=4.082e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 8.625e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,432:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=3.317e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,432:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=3.116e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,433:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.956e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,434:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.140e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,436:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=6.347e-03, with an active set of 54 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,438:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:18,464:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=2.077e+00, with an active set of 42 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,465:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:18,466:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=4.003e+00, with an active set of 47 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,469:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.528e+00, with an active set of 47 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,471:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.999e+00, with an active set of 48 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,473:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.119e+00, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,474:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.670e+00, with an active set of 54 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,475:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.329e+00, with an active set of 54 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,475:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.322e+00, with an active set of 54 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,492:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=3.044e+01, with an active set of 48 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,494:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.164e+01, with an active set of 51 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,495:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.742e+01, with an active set of 51 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,496:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.021e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,498:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=6.889e+00, with an active set of 54 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,498:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=4.416e+00, with an active set of 54 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:18,498:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=7.214e-01, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:19,878:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:19,881:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:19,889:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.951e+00, with an active set of 48 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:19,891:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=5.202e-01, with an active set of 55 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:19,892:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.002e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:19,893:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=3.701e+06, with an active set of 44 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:19,895:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.528e+06, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:19,896:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.473e+06, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:19,896:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.514e+05, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:19,896:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.264e+05, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:19,897:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=2.873e+05, with an active set of 53 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:19,897:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=2.023e+05, with an active set of 54 regressors, and the smallest cholesky pivot element being 9.884e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:19,897:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=2.092e+04, with an active set of 54 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-04-03 18:43:20,396:INFO:Calculating mean and std
2023-04-03 18:43:20,398:INFO:Creating metrics dataframe
2023-04-03 18:43:20,488:INFO:Uploading results into container
2023-04-03 18:43:20,488:INFO:Uploading model into container now
2023-04-03 18:43:20,489:INFO:_master_model_container: 5
2023-04-03 18:43:20,489:INFO:_display_container: 2
2023-04-03 18:43:20,489:INFO:Lars(random_state=1797)
2023-04-03 18:43:20,489:INFO:create_model() successfully completed......................................
2023-04-03 18:43:20,658:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:20,658:INFO:Creating metrics dataframe
2023-04-03 18:43:20,678:INFO:Initializing Lasso Least Angle Regression
2023-04-03 18:43:20,678:INFO:Total runtime is 0.6915022055308024 minutes
2023-04-03 18:43:20,684:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:20,686:INFO:Initializing create_model()
2023-04-03 18:43:20,687:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:20,687:INFO:Checking exceptions
2023-04-03 18:43:20,687:INFO:Importing libraries
2023-04-03 18:43:20,687:INFO:Copying training dataset
2023-04-03 18:43:20,696:INFO:Defining folds
2023-04-03 18:43:20,696:INFO:Declaring metric variables
2023-04-03 18:43:20,703:INFO:Importing untrained model
2023-04-03 18:43:20,711:INFO:Lasso Least Angle Regression Imported successfully
2023-04-03 18:43:20,722:INFO:Starting cross validation
2023-04-03 18:43:20,726:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:21,426:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-04-03 18:43:21,431:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-04-03 18:43:21,490:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-04-03 18:43:21,625:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-04-03 18:43:21,629:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-04-03 18:43:21,632:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-04-03 18:43:21,633:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-04-03 18:43:21,780:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-04-03 18:43:22,999:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-04-03 18:43:23,045:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-04-03 18:43:23,517:INFO:Calculating mean and std
2023-04-03 18:43:23,520:INFO:Creating metrics dataframe
2023-04-03 18:43:23,609:INFO:Uploading results into container
2023-04-03 18:43:23,610:INFO:Uploading model into container now
2023-04-03 18:43:23,610:INFO:_master_model_container: 6
2023-04-03 18:43:23,610:INFO:_display_container: 2
2023-04-03 18:43:23,611:INFO:LassoLars(random_state=1797)
2023-04-03 18:43:23,611:INFO:create_model() successfully completed......................................
2023-04-03 18:43:23,788:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:23,788:INFO:Creating metrics dataframe
2023-04-03 18:43:23,800:INFO:Initializing Orthogonal Matching Pursuit
2023-04-03 18:43:23,801:INFO:Total runtime is 0.7435429255167644 minutes
2023-04-03 18:43:23,808:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:23,808:INFO:Initializing create_model()
2023-04-03 18:43:23,809:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:23,809:INFO:Checking exceptions
2023-04-03 18:43:23,810:INFO:Importing libraries
2023-04-03 18:43:23,810:INFO:Copying training dataset
2023-04-03 18:43:23,820:INFO:Defining folds
2023-04-03 18:43:23,820:INFO:Declaring metric variables
2023-04-03 18:43:23,827:INFO:Importing untrained model
2023-04-03 18:43:23,832:INFO:Orthogonal Matching Pursuit Imported successfully
2023-04-03 18:43:23,847:INFO:Starting cross validation
2023-04-03 18:43:23,851:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:24,522:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:24,556:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:24,577:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:24,594:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:24,617:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:24,651:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:24,698:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:24,758:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:26,083:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:26,110:WARNING:c:\Anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-04-03 18:43:26,631:INFO:Calculating mean and std
2023-04-03 18:43:26,633:INFO:Creating metrics dataframe
2023-04-03 18:43:26,726:INFO:Uploading results into container
2023-04-03 18:43:26,728:INFO:Uploading model into container now
2023-04-03 18:43:26,728:INFO:_master_model_container: 7
2023-04-03 18:43:26,728:INFO:_display_container: 2
2023-04-03 18:43:26,728:INFO:OrthogonalMatchingPursuit()
2023-04-03 18:43:26,728:INFO:create_model() successfully completed......................................
2023-04-03 18:43:26,917:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:26,917:INFO:Creating metrics dataframe
2023-04-03 18:43:26,941:INFO:Initializing Bayesian Ridge
2023-04-03 18:43:26,941:INFO:Total runtime is 0.7958737174669902 minutes
2023-04-03 18:43:26,947:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:26,948:INFO:Initializing create_model()
2023-04-03 18:43:26,948:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:26,948:INFO:Checking exceptions
2023-04-03 18:43:26,949:INFO:Importing libraries
2023-04-03 18:43:26,949:INFO:Copying training dataset
2023-04-03 18:43:26,959:INFO:Defining folds
2023-04-03 18:43:26,960:INFO:Declaring metric variables
2023-04-03 18:43:26,966:INFO:Importing untrained model
2023-04-03 18:43:26,976:INFO:Bayesian Ridge Imported successfully
2023-04-03 18:43:26,987:INFO:Starting cross validation
2023-04-03 18:43:26,991:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:29,881:INFO:Calculating mean and std
2023-04-03 18:43:29,883:INFO:Creating metrics dataframe
2023-04-03 18:43:29,980:INFO:Uploading results into container
2023-04-03 18:43:29,981:INFO:Uploading model into container now
2023-04-03 18:43:29,982:INFO:_master_model_container: 8
2023-04-03 18:43:29,982:INFO:_display_container: 2
2023-04-03 18:43:29,982:INFO:BayesianRidge()
2023-04-03 18:43:29,982:INFO:create_model() successfully completed......................................
2023-04-03 18:43:30,162:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:30,162:INFO:Creating metrics dataframe
2023-04-03 18:43:30,179:INFO:Initializing Passive Aggressive Regressor
2023-04-03 18:43:30,180:INFO:Total runtime is 0.8498635093371074 minutes
2023-04-03 18:43:30,187:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:30,188:INFO:Initializing create_model()
2023-04-03 18:43:30,188:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:30,188:INFO:Checking exceptions
2023-04-03 18:43:30,188:INFO:Importing libraries
2023-04-03 18:43:30,188:INFO:Copying training dataset
2023-04-03 18:43:30,201:INFO:Defining folds
2023-04-03 18:43:30,201:INFO:Declaring metric variables
2023-04-03 18:43:30,211:INFO:Importing untrained model
2023-04-03 18:43:30,217:INFO:Passive Aggressive Regressor Imported successfully
2023-04-03 18:43:30,231:INFO:Starting cross validation
2023-04-03 18:43:30,239:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:33,012:INFO:Calculating mean and std
2023-04-03 18:43:33,014:INFO:Creating metrics dataframe
2023-04-03 18:43:33,114:INFO:Uploading results into container
2023-04-03 18:43:33,115:INFO:Uploading model into container now
2023-04-03 18:43:33,116:INFO:_master_model_container: 9
2023-04-03 18:43:33,116:INFO:_display_container: 2
2023-04-03 18:43:33,116:INFO:PassiveAggressiveRegressor(random_state=1797)
2023-04-03 18:43:33,117:INFO:create_model() successfully completed......................................
2023-04-03 18:43:33,299:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:33,299:INFO:Creating metrics dataframe
2023-04-03 18:43:33,314:INFO:Initializing Huber Regressor
2023-04-03 18:43:33,315:INFO:Total runtime is 0.9021108627319336 minutes
2023-04-03 18:43:33,323:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:33,324:INFO:Initializing create_model()
2023-04-03 18:43:33,324:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:33,324:INFO:Checking exceptions
2023-04-03 18:43:33,325:INFO:Importing libraries
2023-04-03 18:43:33,325:INFO:Copying training dataset
2023-04-03 18:43:33,332:INFO:Defining folds
2023-04-03 18:43:33,334:INFO:Declaring metric variables
2023-04-03 18:43:33,341:INFO:Importing untrained model
2023-04-03 18:43:33,349:INFO:Huber Regressor Imported successfully
2023-04-03 18:43:33,362:INFO:Starting cross validation
2023-04-03 18:43:33,367:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:36,213:INFO:Calculating mean and std
2023-04-03 18:43:36,216:INFO:Creating metrics dataframe
2023-04-03 18:43:36,340:INFO:Uploading results into container
2023-04-03 18:43:36,341:INFO:Uploading model into container now
2023-04-03 18:43:36,342:INFO:_master_model_container: 10
2023-04-03 18:43:36,342:INFO:_display_container: 2
2023-04-03 18:43:36,342:INFO:HuberRegressor()
2023-04-03 18:43:36,342:INFO:create_model() successfully completed......................................
2023-04-03 18:43:36,507:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:36,507:INFO:Creating metrics dataframe
2023-04-03 18:43:36,525:INFO:Initializing K Neighbors Regressor
2023-04-03 18:43:36,526:INFO:Total runtime is 0.9556261579195658 minutes
2023-04-03 18:43:36,531:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:36,532:INFO:Initializing create_model()
2023-04-03 18:43:36,533:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:36,533:INFO:Checking exceptions
2023-04-03 18:43:36,533:INFO:Importing libraries
2023-04-03 18:43:36,533:INFO:Copying training dataset
2023-04-03 18:43:36,546:INFO:Defining folds
2023-04-03 18:43:36,546:INFO:Declaring metric variables
2023-04-03 18:43:36,554:INFO:Importing untrained model
2023-04-03 18:43:36,561:INFO:K Neighbors Regressor Imported successfully
2023-04-03 18:43:36,576:INFO:Starting cross validation
2023-04-03 18:43:36,581:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:39,449:INFO:Calculating mean and std
2023-04-03 18:43:39,451:INFO:Creating metrics dataframe
2023-04-03 18:43:39,568:INFO:Uploading results into container
2023-04-03 18:43:39,569:INFO:Uploading model into container now
2023-04-03 18:43:39,571:INFO:_master_model_container: 11
2023-04-03 18:43:39,571:INFO:_display_container: 2
2023-04-03 18:43:39,572:INFO:KNeighborsRegressor(n_jobs=-1)
2023-04-03 18:43:39,572:INFO:create_model() successfully completed......................................
2023-04-03 18:43:39,747:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:39,748:INFO:Creating metrics dataframe
2023-04-03 18:43:39,766:INFO:Initializing Decision Tree Regressor
2023-04-03 18:43:39,766:INFO:Total runtime is 1.0096393704414368 minutes
2023-04-03 18:43:39,776:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:39,777:INFO:Initializing create_model()
2023-04-03 18:43:39,777:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:39,777:INFO:Checking exceptions
2023-04-03 18:43:39,777:INFO:Importing libraries
2023-04-03 18:43:39,777:INFO:Copying training dataset
2023-04-03 18:43:39,784:INFO:Defining folds
2023-04-03 18:43:39,784:INFO:Declaring metric variables
2023-04-03 18:43:39,792:INFO:Importing untrained model
2023-04-03 18:43:39,801:INFO:Decision Tree Regressor Imported successfully
2023-04-03 18:43:39,823:INFO:Starting cross validation
2023-04-03 18:43:39,828:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:42,853:INFO:Calculating mean and std
2023-04-03 18:43:42,858:INFO:Creating metrics dataframe
2023-04-03 18:43:42,973:INFO:Uploading results into container
2023-04-03 18:43:42,974:INFO:Uploading model into container now
2023-04-03 18:43:42,974:INFO:_master_model_container: 12
2023-04-03 18:43:42,974:INFO:_display_container: 2
2023-04-03 18:43:42,975:INFO:DecisionTreeRegressor(random_state=1797)
2023-04-03 18:43:42,975:INFO:create_model() successfully completed......................................
2023-04-03 18:43:43,145:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:43,145:INFO:Creating metrics dataframe
2023-04-03 18:43:43,163:INFO:Initializing Random Forest Regressor
2023-04-03 18:43:43,164:INFO:Total runtime is 1.0662675539652506 minutes
2023-04-03 18:43:43,169:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:43,170:INFO:Initializing create_model()
2023-04-03 18:43:43,172:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:43,172:INFO:Checking exceptions
2023-04-03 18:43:43,172:INFO:Importing libraries
2023-04-03 18:43:43,172:INFO:Copying training dataset
2023-04-03 18:43:43,181:INFO:Defining folds
2023-04-03 18:43:43,182:INFO:Declaring metric variables
2023-04-03 18:43:43,190:INFO:Importing untrained model
2023-04-03 18:43:43,198:INFO:Random Forest Regressor Imported successfully
2023-04-03 18:43:43,211:INFO:Starting cross validation
2023-04-03 18:43:43,215:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:45,477:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-03 18:43:46,068:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-03 18:43:46,156:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-03 18:43:46,158:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-03 18:43:46,567:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-03 18:43:46,599:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-03 18:43:46,609:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-03 18:43:46,620:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-03 18:43:49,718:INFO:Calculating mean and std
2023-04-03 18:43:49,722:INFO:Creating metrics dataframe
2023-04-03 18:43:49,849:INFO:Uploading results into container
2023-04-03 18:43:49,849:INFO:Uploading model into container now
2023-04-03 18:43:49,850:INFO:_master_model_container: 13
2023-04-03 18:43:49,850:INFO:_display_container: 2
2023-04-03 18:43:49,850:INFO:RandomForestRegressor(n_jobs=-1, random_state=1797)
2023-04-03 18:43:49,851:INFO:create_model() successfully completed......................................
2023-04-03 18:43:50,023:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:50,023:INFO:Creating metrics dataframe
2023-04-03 18:43:50,044:INFO:Initializing Extra Trees Regressor
2023-04-03 18:43:50,044:INFO:Total runtime is 1.1809238870938619 minutes
2023-04-03 18:43:50,050:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:50,051:INFO:Initializing create_model()
2023-04-03 18:43:50,051:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:50,051:INFO:Checking exceptions
2023-04-03 18:43:50,051:INFO:Importing libraries
2023-04-03 18:43:50,051:INFO:Copying training dataset
2023-04-03 18:43:50,064:INFO:Defining folds
2023-04-03 18:43:50,064:INFO:Declaring metric variables
2023-04-03 18:43:50,070:INFO:Importing untrained model
2023-04-03 18:43:50,079:INFO:Extra Trees Regressor Imported successfully
2023-04-03 18:43:50,091:INFO:Starting cross validation
2023-04-03 18:43:50,097:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:43:51,551:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-03 18:43:51,767:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-04-03 18:43:52,332:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-03 18:43:52,751:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-03 18:43:52,791:WARNING:C:\Users\Playdata\AppData\Roaming\Python\Python39\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-04-03 18:43:56,137:INFO:Calculating mean and std
2023-04-03 18:43:56,140:INFO:Creating metrics dataframe
2023-04-03 18:43:56,273:INFO:Uploading results into container
2023-04-03 18:43:56,275:INFO:Uploading model into container now
2023-04-03 18:43:56,277:INFO:_master_model_container: 14
2023-04-03 18:43:56,277:INFO:_display_container: 2
2023-04-03 18:43:56,278:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=1797)
2023-04-03 18:43:56,278:INFO:create_model() successfully completed......................................
2023-04-03 18:43:56,446:INFO:SubProcess create_model() end ==================================
2023-04-03 18:43:56,446:INFO:Creating metrics dataframe
2023-04-03 18:43:56,465:INFO:Initializing AdaBoost Regressor
2023-04-03 18:43:56,466:INFO:Total runtime is 1.2879697680473328 minutes
2023-04-03 18:43:56,472:INFO:SubProcess create_model() called ==================================
2023-04-03 18:43:56,473:INFO:Initializing create_model()
2023-04-03 18:43:56,474:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:43:56,474:INFO:Checking exceptions
2023-04-03 18:43:56,474:INFO:Importing libraries
2023-04-03 18:43:56,474:INFO:Copying training dataset
2023-04-03 18:43:56,486:INFO:Defining folds
2023-04-03 18:43:56,487:INFO:Declaring metric variables
2023-04-03 18:43:56,497:INFO:Importing untrained model
2023-04-03 18:43:56,505:INFO:AdaBoost Regressor Imported successfully
2023-04-03 18:43:56,520:INFO:Starting cross validation
2023-04-03 18:43:56,525:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:44:00,770:INFO:Calculating mean and std
2023-04-03 18:44:00,772:INFO:Creating metrics dataframe
2023-04-03 18:44:00,960:INFO:Uploading results into container
2023-04-03 18:44:00,961:INFO:Uploading model into container now
2023-04-03 18:44:00,962:INFO:_master_model_container: 15
2023-04-03 18:44:00,962:INFO:_display_container: 2
2023-04-03 18:44:00,962:INFO:AdaBoostRegressor(random_state=1797)
2023-04-03 18:44:00,962:INFO:create_model() successfully completed......................................
2023-04-03 18:44:01,130:INFO:SubProcess create_model() end ==================================
2023-04-03 18:44:01,130:INFO:Creating metrics dataframe
2023-04-03 18:44:01,147:INFO:Initializing Gradient Boosting Regressor
2023-04-03 18:44:01,148:INFO:Total runtime is 1.3659759561220806 minutes
2023-04-03 18:44:01,156:INFO:SubProcess create_model() called ==================================
2023-04-03 18:44:01,158:INFO:Initializing create_model()
2023-04-03 18:44:01,158:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:44:01,159:INFO:Checking exceptions
2023-04-03 18:44:01,159:INFO:Importing libraries
2023-04-03 18:44:01,159:INFO:Copying training dataset
2023-04-03 18:44:01,170:INFO:Defining folds
2023-04-03 18:44:01,170:INFO:Declaring metric variables
2023-04-03 18:44:01,178:INFO:Importing untrained model
2023-04-03 18:44:01,186:INFO:Gradient Boosting Regressor Imported successfully
2023-04-03 18:44:01,199:INFO:Starting cross validation
2023-04-03 18:44:01,204:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:44:06,282:INFO:Calculating mean and std
2023-04-03 18:44:06,284:INFO:Creating metrics dataframe
2023-04-03 18:44:06,451:INFO:Uploading results into container
2023-04-03 18:44:06,451:INFO:Uploading model into container now
2023-04-03 18:44:06,452:INFO:_master_model_container: 16
2023-04-03 18:44:06,452:INFO:_display_container: 2
2023-04-03 18:44:06,452:INFO:GradientBoostingRegressor(random_state=1797)
2023-04-03 18:44:06,452:INFO:create_model() successfully completed......................................
2023-04-03 18:44:06,622:INFO:SubProcess create_model() end ==================================
2023-04-03 18:44:06,623:INFO:Creating metrics dataframe
2023-04-03 18:44:06,645:INFO:Initializing Extreme Gradient Boosting
2023-04-03 18:44:06,645:INFO:Total runtime is 1.4576115171114605 minutes
2023-04-03 18:44:06,650:INFO:SubProcess create_model() called ==================================
2023-04-03 18:44:06,651:INFO:Initializing create_model()
2023-04-03 18:44:06,651:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:44:06,651:INFO:Checking exceptions
2023-04-03 18:44:06,651:INFO:Importing libraries
2023-04-03 18:44:06,652:INFO:Copying training dataset
2023-04-03 18:44:06,665:INFO:Defining folds
2023-04-03 18:44:06,666:INFO:Declaring metric variables
2023-04-03 18:44:06,676:INFO:Importing untrained model
2023-04-03 18:44:06,681:INFO:Extreme Gradient Boosting Imported successfully
2023-04-03 18:44:06,695:INFO:Starting cross validation
2023-04-03 18:44:06,700:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:44:12,032:INFO:Calculating mean and std
2023-04-03 18:44:12,035:INFO:Creating metrics dataframe
2023-04-03 18:44:12,207:INFO:Uploading results into container
2023-04-03 18:44:12,209:INFO:Uploading model into container now
2023-04-03 18:44:12,210:INFO:_master_model_container: 17
2023-04-03 18:44:12,211:INFO:_display_container: 2
2023-04-03 18:44:12,213:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=1797, ...)
2023-04-03 18:44:12,214:INFO:create_model() successfully completed......................................
2023-04-03 18:44:12,391:INFO:SubProcess create_model() end ==================================
2023-04-03 18:44:12,392:INFO:Creating metrics dataframe
2023-04-03 18:44:12,415:INFO:Initializing Light Gradient Boosting Machine
2023-04-03 18:44:12,416:INFO:Total runtime is 1.5537848075230918 minutes
2023-04-03 18:44:12,421:INFO:SubProcess create_model() called ==================================
2023-04-03 18:44:12,422:INFO:Initializing create_model()
2023-04-03 18:44:12,422:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:44:12,423:INFO:Checking exceptions
2023-04-03 18:44:12,423:INFO:Importing libraries
2023-04-03 18:44:12,423:INFO:Copying training dataset
2023-04-03 18:44:12,437:INFO:Defining folds
2023-04-03 18:44:12,437:INFO:Declaring metric variables
2023-04-03 18:44:12,443:INFO:Importing untrained model
2023-04-03 18:44:12,454:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-03 18:44:12,471:INFO:Starting cross validation
2023-04-03 18:44:12,477:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:44:20,595:INFO:Calculating mean and std
2023-04-03 18:44:20,597:INFO:Creating metrics dataframe
2023-04-03 18:44:20,831:INFO:Uploading results into container
2023-04-03 18:44:20,832:INFO:Uploading model into container now
2023-04-03 18:44:20,833:INFO:_master_model_container: 18
2023-04-03 18:44:20,833:INFO:_display_container: 2
2023-04-03 18:44:20,834:INFO:LGBMRegressor(random_state=1797)
2023-04-03 18:44:20,834:INFO:create_model() successfully completed......................................
2023-04-03 18:44:21,024:INFO:SubProcess create_model() end ==================================
2023-04-03 18:44:21,025:INFO:Creating metrics dataframe
2023-04-03 18:44:21,049:INFO:Initializing Dummy Regressor
2023-04-03 18:44:21,049:INFO:Total runtime is 1.6976814826329552 minutes
2023-04-03 18:44:21,055:INFO:SubProcess create_model() called ==================================
2023-04-03 18:44:21,056:INFO:Initializing create_model()
2023-04-03 18:44:21,056:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016FFBE6A250>, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:44:21,056:INFO:Checking exceptions
2023-04-03 18:44:21,056:INFO:Importing libraries
2023-04-03 18:44:21,057:INFO:Copying training dataset
2023-04-03 18:44:21,072:INFO:Defining folds
2023-04-03 18:44:21,072:INFO:Declaring metric variables
2023-04-03 18:44:21,083:INFO:Importing untrained model
2023-04-03 18:44:21,090:INFO:Dummy Regressor Imported successfully
2023-04-03 18:44:21,105:INFO:Starting cross validation
2023-04-03 18:44:21,112:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-04-03 18:44:24,691:INFO:Calculating mean and std
2023-04-03 18:44:24,696:INFO:Creating metrics dataframe
2023-04-03 18:44:24,958:INFO:Uploading results into container
2023-04-03 18:44:24,960:INFO:Uploading model into container now
2023-04-03 18:44:24,962:INFO:_master_model_container: 19
2023-04-03 18:44:24,963:INFO:_display_container: 2
2023-04-03 18:44:24,964:INFO:DummyRegressor()
2023-04-03 18:44:24,964:INFO:create_model() successfully completed......................................
2023-04-03 18:44:25,139:INFO:SubProcess create_model() end ==================================
2023-04-03 18:44:25,139:INFO:Creating metrics dataframe
2023-04-03 18:44:25,183:INFO:Initializing create_model()
2023-04-03 18:44:25,183:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=1797), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:44:25,184:INFO:Checking exceptions
2023-04-03 18:44:25,186:INFO:Importing libraries
2023-04-03 18:44:25,186:INFO:Copying training dataset
2023-04-03 18:44:25,201:INFO:Defining folds
2023-04-03 18:44:25,201:INFO:Declaring metric variables
2023-04-03 18:44:25,201:INFO:Importing untrained model
2023-04-03 18:44:25,201:INFO:Declaring custom model
2023-04-03 18:44:25,202:INFO:Extra Trees Regressor Imported successfully
2023-04-03 18:44:25,205:INFO:Cross validation set to False
2023-04-03 18:44:25,205:INFO:Fitting Model
2023-04-03 18:44:26,518:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=1797)
2023-04-03 18:44:26,518:INFO:create_model() successfully completed......................................
2023-04-03 18:44:26,706:INFO:Initializing create_model()
2023-04-03 18:44:26,706:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=RandomForestRegressor(n_jobs=-1, random_state=1797), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:44:26,707:INFO:Checking exceptions
2023-04-03 18:44:26,712:INFO:Importing libraries
2023-04-03 18:44:26,713:INFO:Copying training dataset
2023-04-03 18:44:26,719:INFO:Defining folds
2023-04-03 18:44:26,720:INFO:Declaring metric variables
2023-04-03 18:44:26,720:INFO:Importing untrained model
2023-04-03 18:44:26,720:INFO:Declaring custom model
2023-04-03 18:44:26,721:INFO:Random Forest Regressor Imported successfully
2023-04-03 18:44:26,722:INFO:Cross validation set to False
2023-04-03 18:44:26,722:INFO:Fitting Model
2023-04-03 18:44:27,891:INFO:RandomForestRegressor(n_jobs=-1, random_state=1797)
2023-04-03 18:44:27,892:INFO:create_model() successfully completed......................................
2023-04-03 18:44:28,082:INFO:Initializing create_model()
2023-04-03 18:44:28,083:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=GradientBoostingRegressor(random_state=1797), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:44:28,083:INFO:Checking exceptions
2023-04-03 18:44:28,087:INFO:Importing libraries
2023-04-03 18:44:28,087:INFO:Copying training dataset
2023-04-03 18:44:28,097:INFO:Defining folds
2023-04-03 18:44:28,097:INFO:Declaring metric variables
2023-04-03 18:44:28,097:INFO:Importing untrained model
2023-04-03 18:44:28,097:INFO:Declaring custom model
2023-04-03 18:44:28,098:INFO:Gradient Boosting Regressor Imported successfully
2023-04-03 18:44:28,101:INFO:Cross validation set to False
2023-04-03 18:44:28,101:INFO:Fitting Model
2023-04-03 18:44:29,314:INFO:GradientBoostingRegressor(random_state=1797)
2023-04-03 18:44:29,314:INFO:create_model() successfully completed......................................
2023-04-03 18:44:29,521:INFO:Initializing create_model()
2023-04-03 18:44:29,521:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=DecisionTreeRegressor(random_state=1797), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:44:29,522:INFO:Checking exceptions
2023-04-03 18:44:29,526:INFO:Importing libraries
2023-04-03 18:44:29,526:INFO:Copying training dataset
2023-04-03 18:44:29,535:INFO:Defining folds
2023-04-03 18:44:29,535:INFO:Declaring metric variables
2023-04-03 18:44:29,535:INFO:Importing untrained model
2023-04-03 18:44:29,535:INFO:Declaring custom model
2023-04-03 18:44:29,536:INFO:Decision Tree Regressor Imported successfully
2023-04-03 18:44:29,538:INFO:Cross validation set to False
2023-04-03 18:44:29,538:INFO:Fitting Model
2023-04-03 18:44:30,183:INFO:DecisionTreeRegressor(random_state=1797)
2023-04-03 18:44:30,183:INFO:create_model() successfully completed......................................
2023-04-03 18:44:30,385:INFO:Initializing create_model()
2023-04-03 18:44:30,385:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016FFBD3FD60>, estimator=LGBMRegressor(random_state=1797), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-04-03 18:44:30,385:INFO:Checking exceptions
2023-04-03 18:44:30,390:INFO:Importing libraries
2023-04-03 18:44:30,390:INFO:Copying training dataset
2023-04-03 18:44:30,401:INFO:Defining folds
2023-04-03 18:44:30,401:INFO:Declaring metric variables
2023-04-03 18:44:30,401:INFO:Importing untrained model
2023-04-03 18:44:30,401:INFO:Declaring custom model
2023-04-03 18:44:30,403:INFO:Light Gradient Boosting Machine Imported successfully
2023-04-03 18:44:30,406:INFO:Cross validation set to False
2023-04-03 18:44:30,406:INFO:Fitting Model
2023-04-03 18:44:31,235:INFO:LGBMRegressor(random_state=1797)
2023-04-03 18:44:31,235:INFO:create_model() successfully completed......................................
2023-04-03 18:44:31,463:INFO:_master_model_container: 19
2023-04-03 18:44:31,464:INFO:_display_container: 2
2023-04-03 18:44:31,466:INFO:[ExtraTreesRegressor(n_jobs=-1, random_state=1797), RandomForestRegressor(n_jobs=-1, random_state=1797), GradientBoostingRegressor(random_state=1797), DecisionTreeRegressor(random_state=1797), LGBMRegressor(random_state=1797)]
2023-04-03 18:44:31,466:INFO:compare_models() successfully completed......................................
